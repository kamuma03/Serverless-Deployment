{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd03fdfe149d2ba6488499ebaa9fe03c93cea609de4e6d37e2d6cd3ac6deda6390f",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Serverless Deployment of a machine learning model in AWS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Machine learning model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, plot_confusion_matrix, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# creating train and test dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Forest Classifier number of estimators 50 max depth 5 training dataset F1 Score is 0.9934065934065934 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 50 max depth 10 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 50 max depth 25 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 50 max depth 50 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 50 max depth 100 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 50 max depth 200 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 5 training dataset F1 Score is 0.9912087912087912 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 10 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 25 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 50 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 100 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 200 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 5 training dataset F1 Score is 0.9912087912087912 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 10 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 25 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 50 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 150 max depth 100 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 200 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 200 max depth 5 training dataset F1 Score is 0.9912087912087912 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 200 max depth 10 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 200 max depth 25 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 200 max depth 50 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9649122807017544\n",
      "Random Forest Classifier number of estimators 200 max depth 100 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 200 max depth 200 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "# Optimum model search for the classification\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "max_depth = [5, 10, 25, 50, 100, 200]\n",
    "\n",
    "for i in range(len(n_estimators)):\n",
    "    for j in range(len(max_depth)):\n",
    "        classifier = RandomForestClassifier(n_estimators=n_estimators[i], max_depth=max_depth[j])\n",
    "        classifier.fit(x_train, y_train)\n",
    "        y_pred = classifier.predict(x_train)\n",
    "        f1_score_t = f1_score(y_train, y_pred, average='micro')\n",
    "        y_pred_test = classifier.predict(x_test)\n",
    "        f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "        print_out = 'Random Forest Classifier number of estimators ' +  str(n_estimators[i]) +  ' max depth ' + str(max_depth[j]) + ' training dataset F1 Score is ' + str(f1_score_t) + ' validation dataset F1 Score is ' + str(f1_score_test)\n",
    "        print(print_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.95      0.97      0.96        39\n           1       0.99      0.97      0.98        75\n\n    accuracy                           0.97       114\n   macro avg       0.97      0.97      0.97       114\nweighted avg       0.97      0.97      0.97       114\n\n"
     ]
    }
   ],
   "source": [
    "# Best model for the classification\n",
    "classifier = RandomForestClassifier(n_estimators=50, max_depth=5)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the model with pickle\n",
    "pickle.dump( classifier, open( \"classification_model.p\", \"wb\" ) )"
   ]
  },
  {
   "source": [
    "## Deployment of the model to AWS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "The model\n",
    "\n",
    "- Copy the pickled model to S3 in AWS through command line in the same directory as your notebook run : \"aws s3 cp classification_model.p s3://kashifs3bucket\" with the S3 bucket name at the end\n",
    "- To deploy the model will use SAM “Serverless Application Model”. It’s an open-source framework for provisioning AWS services via code.\n",
    "- Run \"SAM init\" in the comman line in the same directory as your notebook\n",
    "    - Choosing an AWS quick start template\n",
    "    - Package type as zip\n",
    "    - Runtime as python3.8\n",
    "    - Project name\n",
    "    - and as template Hello World Example\n",
    "    - Chnaged the name of the hell_world folder to code\n",
    "- Update the template.yaml document for the required resources in AWS \n",
    "    - By creating the lambda function \n",
    "    - Letting know from where to download the model\n",
    "    - Creating API gateway to connect to the Lambda function from externally\n",
    "    - Need to create the role for lambda function to access the S3 resources\n",
    "- Modify the app.py in the code folder to appropriately handle the lambdda function\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}