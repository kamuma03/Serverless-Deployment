{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd03fdfe149d2ba6488499ebaa9fe03c93cea609de4e6d37e2d6cd3ac6deda6390f",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Serverless Deployment of a machine learning model in AWS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Machine learning model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, plot_confusion_matrix, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# creating train and test dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Forest Classifier number of estimators 50 max depth 5 training dataset F1 Score is 0.9934065934065934 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 50 max depth 10 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 50 max depth 25 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 50 max depth 50 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 50 max depth 100 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 50 max depth 200 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 5 training dataset F1 Score is 0.9912087912087912 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 10 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 25 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 50 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 100 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 100 max depth 200 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 5 training dataset F1 Score is 0.9912087912087912 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 10 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 25 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 50 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 150 max depth 100 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 150 max depth 200 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 200 max depth 5 training dataset F1 Score is 0.9912087912087912 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 200 max depth 10 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 200 max depth 25 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9736842105263158\n",
      "Random Forest Classifier number of estimators 200 max depth 50 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9649122807017544\n",
      "Random Forest Classifier number of estimators 200 max depth 100 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n",
      "Random Forest Classifier number of estimators 200 max depth 200 training dataset F1 Score is 1.0 validation dataset F1 Score is 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "# Optimum model search for the classification\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "max_depth = [5, 10, 25, 50, 100, 200]\n",
    "\n",
    "for i in range(len(n_estimators)):\n",
    "    for j in range(len(max_depth)):\n",
    "        classifier = RandomForestClassifier(n_estimators=n_estimators[i], max_depth=max_depth[j])\n",
    "        classifier.fit(x_train, y_train)\n",
    "        y_pred = classifier.predict(x_train)\n",
    "        f1_score_t = f1_score(y_train, y_pred, average='micro')\n",
    "        y_pred_test = classifier.predict(x_test)\n",
    "        f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "        print_out = 'Random Forest Classifier number of estimators ' +  str(n_estimators[i]) +  ' max depth ' + str(max_depth[j]) + ' training dataset F1 Score is ' + str(f1_score_t) + ' validation dataset F1 Score is ' + str(f1_score_test)\n",
    "        print(print_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.95      0.97      0.96        39\n           1       0.99      0.97      0.98        75\n\n    accuracy                           0.97       114\n   macro avg       0.97      0.97      0.97       114\nweighted avg       0.97      0.97      0.97       114\n\n"
     ]
    }
   ],
   "source": [
    "# Best model for the classification\n",
    "classifier = RandomForestClassifier(n_estimators=50, max_depth=5)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the model with pickle\n",
    "pickle.dump( classifier, open( \"classification_model.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.546e+01 1.189e+01 1.025e+02 7.369e+02 1.257e-01 1.555e-01 2.032e-01\n 1.097e-01 1.966e-01 7.069e-02 4.209e-01 6.583e-01 2.805e+00 4.464e+01\n 5.393e-03 2.321e-02 4.303e-02 1.320e-02 1.792e-02 4.168e-03 1.879e+01\n 1.704e+01 1.250e+02 1.102e+03 1.531e-01 3.583e-01 5.830e-01 1.827e-01\n 3.216e-01 1.010e-01]\n0\n"
     ]
    }
   ],
   "source": [
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Deployment of the model to AWS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I have used SAM for the model deployment.\n",
    "\n",
    "SAM stands for “Serverless Application Model”. It’s an open-source framework for provisioning AWS services via code.\n",
    "\n",
    "I have chosen to use SAM because it's more maintainable, reproducible, and allows storing versions in Github or any other simila tools, compeared to setting up infrastructure in the AWS console."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Deployment steps\n",
    "\n",
    "- Copy the pickled model to S3 in AWS through command line in the same directory as your notebook run : \"aws s3 cp classification_model.p s3://kashifs3bucket\" with the S3 bucket name at the end\n",
    "- Run \"sam init\" in the comman line in the same directory as your notebook to create for first time and i have chosen the below option for my project\n",
    "    - Choosing an AWS quick start template\n",
    "    - Package type as zip\n",
    "    - Runtime as python3.8\n",
    "    - Project name\n",
    "    - and as template Hello World Example\n",
    "    - Chnaged the name of the hell_world folder to code\n",
    "- Update the template.yaml document for the required resources in AWS \n",
    "    - By creating the lambda function \n",
    "    - Letting know from where to download the model s3 bucket\n",
    "    - Creating API gateway to connect to the Lambda function from externally\n",
    "    - Need to create the role for lambda function to access the S3 resources\n",
    "- Modify the app.py in the code folder to appropriately handle the lambdda function\n",
    "- Run \"sam build\" to build all the modules and will create the folders .aws-sam/build/\n",
    "    - cd into the .aws-sam/build/ folder\n",
    "- Need to create the zip file to deploy\n",
    "    - run \"sam package --template-file template.yaml --s3-bucket kashifs3bucket --output-template-file packaged.yaml\"\n",
    "- To deploy the created function run: \" sam deploy --template-file packaged.yaml --stack-name ClassifierLambdaStack --capabilities CAPABILITY_IAM\"\n",
    "- Once the function has been deployed try to check if the model works by: \" curl -XPOST -g https://inocybgz06.execute-api.eu-west-2.amazonaws.com/Prod/classify/ -H 'Content-Type:application/json' -d \"@test.json\"  \"\n",
    "    - The response should be \" {\"prediction\": \"0\"} \""
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}